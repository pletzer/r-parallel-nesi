#!Jinja2
[scheduler]
{% set NS = 11, 21, 51, 101 %}
[task parameters]
    # run multiple instances
    m = 0..4
[scheduling] # Define the tasks and when they should run
  [[graph]]
    R1 = """ # R1 means run this graph once
      install => serial<m>
      serial<m> => analyse
      {% for N in NS %}
        install => shrd{{ N }}<m> & dist{{ N }}<m> & distmlx{{ N }}<m> & distsshbroadwell{{ N }}<m>
        shrd{{ N }}<m> & dist{{ N }}<m> & distmlx{{ N }}<m> & distsshbroadwell{{ N }}<m> => analyse
      {% endfor %}
    """
[runtime] # Define what each task should run
  [[root]] # Default settings inherited by all tasks
    platform = mahuika-slurm # Run "cylc conf" to see platforms. 
    [[[directives]]] # Default SLURM options for the tasks below
       --account = nesi99999 # CHANGE
       --hint = nomultithread
       --partition = milan
       --mem = 10gb
    [[[environment]]]
      TOP_DIR="/nesi/nobackup/pletzera/r-parallel-nesi/" # CHANGE
      NTASKS=10000
      BIN_DIR=$CYLC_WORKFLOW_SHARE_DIR/bin
  [[install]]
        script = """
# copy the R scripts to a directory that belongs to the workflow, this allows 
# one to continuously update the scripts without affecting a currently running 
# workflow
          mkdir -p $BIN_DIR
          cp $TOP_DIR/*/*.R $BIN_DIR
          echo "BIN_DIR = $BIN_DIR"
          echo "scripts in $BIN_DIR: $(ls $BIN_DIR)"
        """
        platform = localhost
  [[serial<m>]]
    script = """
          module purge
          module load R
          srun Rscript $BIN_DIR/serial.R $NTASKS
      """
      [[[directives]]] # specific SLURM option for this task
        --ntasks = 1
        --cpus-per-task = 1
  {% for N in NS %}
  [[shrd{{ N }}<m>]]
    script = """
          module purge
          module load R
          srun Rscript $BIN_DIR/shared.R $NTASKS
      """
      [[[directives]]] # specific SLURM option for this task
        --ntasks = 1
        --cpus-per-task = {{ N }}
  [[dist{{ N }}<m>]]
    script = """ 
          module purge 
          module load R impi
          export I_MPI_SPAWN=on
          export I_MPI_PIN_RESPECT_CPUSET=0
          mpirun -bootstrap slurm Rscript $BIN_DIR/distributed.R $NTASKS
      """
      [[[directives]]] # specific SLURM option for this task
        --ntasks = {{ N }}
        --cpus-per-task = 1
  [[distmlx{{ N }}<m>]]
    script = """ 
          module purge 
          module load R impi
          export I_MPI_SPAWN=on
          export I_MPI_PIN_RESPECT_CPUSET=0
          export FI_PROVIDER=mlx
          mpirun -bootstrap slurm Rscript $BIN_DIR/distributed.R $NTASKS
      """
      [[[directives]]] # specific SLURM option for this task
        --ntasks = {{ N }}
        --cpus-per-task = 1
  [[distsshbroadwell{{ N }}<m>]]
    script = """ 
          module purge 
          module load R impi
          export I_MPI_SPAWN=on
          export I_MPI_PIN_RESPECT_CPUSET=0
          mpirun -bootstrap ssh Rscript $BIN_DIR/distributed.R $NTASKS
      """
      [[[directives]]] # specific SLURM option for this task
        --ntasks = {{ N }}
        --cpus-per-task = 1
        --partition = large
  {% endfor %}
  [[analyse]]
    platform = localhost
    script = """
        ml purge
        ml Python
        python analyse.py
    """

